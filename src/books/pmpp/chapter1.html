<!DOCTYPE html>
<html>
<title>Noah Lewis (Personal Website)</title>
<link rel="stylesheet" href="../../styles.css">

<body>
    <h1><a href="/">Noah Lewis (Personal Website)</a></h1>
    <hr>
    <h2>Introduction Answers</h2>
    <p>
        This page provides an answers for chapter 1 (Introduction)
        of the book "Programming Massively Parallel Processor".
    </p>
    <hr>
    <h2>Question & Answers</h2>
    <h3>Problem 1</h3>
    <p>
        If we want to use each thread in a grid to calculate one
        output element of a vector addition, what would be the expression
        for mapping the thread/block indices to the data index (i)?
    </p>
    <li>
        <ol>(A) i = threadIdx.x + threadIdx.y;</ol>
        <ol>(B) i = blockIdx.x + threadIdx.x;</ol>
        <ol>(C) i = blockIdx.x * blockDim.x + threadIdx.x;</ol>
        <ol>(D) i = blockIdx.x * threadIdx.x</ol>
    </li>
    <h3>Answer Problem 2</h3>
    <p>
        (C) i = blockIdx.x * blockDim.x + threadIdx.x;
    </p>
    <h3>Problem 2</h3>
    <p>
        Assume that we want to use each thread to calculate two adjacent
        elements of a vector addition. What would be the expression for
        mapping the thread/block indices to the data index (i) of the first
        element to be processed by a thread?
    </p>
    <li>
        <ol>(A) i = blockIdx.x * blockDim.x + threadIdx.x + 2</ol>
        <ol>(B) i = blockIdx.x * threadIdx.x * 2</ol>
        <ol>(C) i = (blockIdx.x * blockDim.x + threadIdx.x) * 2</ol>
        <ol>(D) i = blockIdx.x * blockDim.x * 2 + threadIdx.x</ol>
    </li>
    <h3>Answer Problem 2</h3>
    <p>
        (C) i = (blockIdx.x * blockDim.x + threadIdx.x) * 2;
    </p>
    <h3>Problem 3</h3>
    <p>
        We want to use each thread to calculate two elements of a
        vector addition. Each thread block processes 2 * blockDim.x
        consecutive elements that form two sections. All threads in
        each block will process a section first, each processing one
        element. They will then all move to the next section, each
        processing one element. Assume that variable i should be the
        index for the first element to be processed by a thread.
        What would be the expression for the first element to be
        processed by a thread. What would be the expression for
        mapping the thread/block indices to data index of the first
        element?
    </p>
    <li>
        <ol>(A) i = blockIdx.x * blockDim.x + threadIdx.x + 2</ol>
        <ol>(B) i = blockIdx.x * threadIdx.x * 2</ol>
        <ol>(C) i = (blockIdx.x * blockDim.x + threadIdx.x) * 2</ol>
        <ol>(D) i = blockIdx.x * blockDim.x * 2 + threadIdx.x</ol>
    </li>
    <h3>Answer Problem 3</h3>
    <p>
        (D) i = blockIdx.x * blockDim.x * 2 + threadIdx.x;
    </p>
    <h3>Problem 4</h3>
    <p>
        For vector addition, assume that the vector length is 8000,
        each thread calculates one output element, and the thread
        block size is 1024 threads. The programmer configures the
        kernel call to have a minimum number of thread blocks to
        cover all output elements. How many threads will be in
        the grid?
    </p>
    <li>
        <ol>(A) 8000</ol>
        <ol>(B) 8196</ol>
        <ol>(C) 8192</ol>
        <ol>(D) 8200</ol>
    </li>
    <h3>Answer Problem 4</h3>
    <p>
        (C) num_threads = ceil(8000 / 1024) * 1024 = 8192;
    </p>
    <h3>Problem 5</h3>
    <p>
        If we want to allocate an array of v integer elements
        in the CUDA device global memory, what would be the
        appropriate expression for the second argument of the
        cudaMalloc call?
    </p>
    <li>
        <ol>(A) n</ol>
        <ol>(B) v</ol>
        <ol>(C) n * sizeof(int)</ol>
        <ol>(D) v * sizeof(int)</ol>
    </li>
    <h3>Answer Problem 5</h3>
    <p>
        (D) v * sizeof(int)
    </p>
    <h3>Problem 6</h3>
    <p>
        If we want to allocate an array of n floating-point
        elements and have a floating-point pointer variable
        A_d to point to the allocated memory, what would
        be an appropriate expression for the first argument
        of the cudaMalloc() call.
    </p>
    <li>
        <ol>(A) n</ol>
        <ol>(B) (void*)A_d</ol>
        <ol>(C) *A_d</ol>
        <ol>(D) (void**)&A_d</ol>
    </li>
    <h3>Answer Problem 6</h3>
    <p>
        (D) (void**)&A_d
    </p>
    <h3>Problem 7</h3>
    <p>
        If we want to copy 300 bytes of data from host array A_h
        (A_h is a pointer to element 0 fo the source array) to
        device array A_d (A_d is a pointer to element 0 of the
        destination array), what would be an appropriate API call
        for this data copy in CUDA?
    </p>
    <li>
        <ol>(A) cudaMemcpy(3000, A_h, A_d, cudaMemcpyHostToDevice)</ol>
        <ol>(B) cudaMemcpy(A_h, A_d, 3000, cudaMemcpyDeviceToHost)</ol>
        <ol>(C) cudaMemcpy(A_d, A_h, 3000, cudaMemcpyHostToDevice)</ol>
        <ol>(D) cudaMemcpy(3000, A_d, A_h, cudaMemcpyHostToDevice)</ol>
    </li>
    <h3>Answer Problem 7</h3>
    <p>
        (C) cudaMemcpy(A_d, A_h, 3000, cudaMemcpyHostToDevice)
    </p>
    <h3>Problem 8</h3>
    <p>
        How would one declare a variable err that can appropriately
        receive the returned value of a CUDA API call?
    </p>
    <li>
        <ol>(A) int err;</ol>
        <ol>(B) cudaError err;</ol>
        <ol>(C) cudaError_t err;</ol>
        <ol>(D) cudaSuccess_t err;</ol>
    </li>
    <h3>Answer Problem 8</h3>
    <p>
        (C) cudaError_t err;
    </p>
    <h3>Problem 9</h3>
    <p>Consider the following CUDA kernel and the corresponding
        host function that calls it:
    </p>
    <pre class="code-block">
__global__ void foo_kernel(float* a, float* b, unsigned int N) {
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;

    if(i &lt; N)
        b[i] = 2.7f * a[i] - 4.3f;
}

void foo(float* a_d, float* b_d) {
    unsigned int N = 200000;
    foo_kernel<<<(N + 128 - 1) / 128, 128>>>(a_d, b_d, N);
}
    </pre>
    <li>
        <ol>(a) What is the number of threads per block?</ol>
        <ol>(b) What is the number of threads in the grid?</ol>
        <ol>(c) What is the number of blocks in the grid?</ol>
        <ol>(d) What is the number of threads that execute the code on line 02?</ol>
        <ol>(e) What is the number of threads that execute teh code on line 04?</ol>
    </li>
    <h3>Answer Problem 9</h3>
    <li>
        <ol>(a) 128 threads per block</ol>
        <ol>(b) 200127 * 128 = 25616256 total threads</ol>
        <ol>(c) 200000 + 128 - 1 = 200127 blocks</ol>
        <ol>(d) 25616256 threads</ol>
        <ol>(e) 200000 threads</ol>
    </li>
    <h3>Problem 10</h3>
    <p>
        A new summer intern was fustrated with CUDA. He has been
        complaining that CUDA is very tedious. He had to declare
        many functions that he plans to execute on both the
        host and the device twice, one as a host function and
        once as a device function. What is your reponse?
    </p>
    <h3>Answer Problem 10</h3>
    <p>
        You should make a single function and prefix the
        function signature with __device__ and __host__.
        This will generate two functions, one which can be
        called from the host and one that can be called from
        the device.
    </p>
</body>

</html>